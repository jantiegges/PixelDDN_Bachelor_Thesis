# Define model parameters

autoencoder:
  LAE:
    variational: False
    convolutional: False
    residual: False
    encoder:
      hidden_layers: 4
      out_features: [256, 256, 256, 256, 256] # first + hidden layer
    decoder:
      hidden_layers: 4
      out_features: [256, 256, 256, 256, 256] # first + hidden layer

  CAE:
    variational: False
    convolutional: True
    residual: False
    encoder:
      hidden_layers: 4
      filters: [32, 64, 128, 256, 512]  # first + hidden layer
      kernel_sizes: [3, 3, 3, 3, 3, 3]  # first + hidden + last layer
      strides: [2, 2, 2, 2, 2, 2]  # first + hidden + last layer
    decoder:
      hidden_layers: 4
      filters: [512, 256, 128, 64, 32]  # first + hidden layer
      kernel_sizes: [3, 3, 3, 3, 3]  # hidden + last layer
      strides: [2, 2, 2, 2]  # hidden layer

  VAE:
    variational: True
    convolutional: False
    residual: False
    encoder:
      hidden_layers: 4
      out_features: [256, 256, 256, 256, 256] # first + hidden layer
    decoder:
      hidden_layers: 4
      out_features: [256, 256, 256, 256, 256] # first + hidden layer

  CVAE:
    variational: True
    convolutional: True
    residual: False
    encoder:
      hidden_layers: 4
      filters: [32, 64, 128, 256, 512]  # first + hidden layer
      kernel_sizes: [3, 3, 3, 3, 3, 3]  # first + hidden + last layer
      strides: [2, 2, 2, 2, 2, 2]  # first + hidden + last layer
    decoder:
      hidden_layers: 4
      filters: [512, 256, 128, 64, 32]  # first + hidden layer
      kernel_sizes: [3, 3, 3, 3, 3]  # hidden + last layer
      strides: [2, 2, 2, 2]  # hidden layer


# activation function options:
#   tanh
#   relu
#   relu^2
#   sigmoid
#   softplus
#   selu
#   elu
#   gelu
#   swish

# integrator options:
#   Euler
#   RK4
#   Leapfrog

dynamics:
  MLP:
    hidden_layers: 2
    activation: "relu" # relu used in final model
  LNN:
    hidden_layers: 2
    # only activation function which have a nonzero second derivative can be used (not relu)
    activation: "relu^2" # relu^2 is meh
    integrator: "Leapfrog"
  HNN:
    hidden_layers: 2
    activation: "relu" # relu used in final model
    integrator: "Euler"
  VIN:
    implemented: "no"
