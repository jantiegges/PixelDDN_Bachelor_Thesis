# activation function for normal Autoencoder (Convolutional is LeakyRelu and can't eb varied)
# options:
#   tanh
#   relu
#   relu^2
#   sigmoid
#   softplus
#   selu
#   elu
#   gelu
#   swish
activation: "relu"

# epochs and batch size for training
epochs: 3
batch_size: 16

# type of loss calculation
# options for VAE & CVAE:
#   normal: normale ELBO
#   beta: https://openreview.net/references/pdf?id=Sy2fzU9gl
#   geco: https://arxiv.org/pdf/1810.00597.pdf
#   additionally one has to choose between mean and sum reduction (example: geco_mean)
#
# options for LAE & CAE:s
#   sum: sum reduction
#   mean: mean reduction
loss_type: 'geco_mean'
beta: 2

# parameter for geco constraint
tol: 1.0e-2
lambd: 1.0
alpha: 0.99

# Learning rates
ae_lr: 1.5e-4
ddn_lr: 1.5e-4
lr: 1.5e-4

# Weigth decay
# not used in final model
#ae_wd: 1.e-4
#ddn_wd: 1.e-4

# max norm for clipping gradients
# not used in final model
#max_norm: 1.0

# number of timesteps that the model should predict
pred_steps: 7